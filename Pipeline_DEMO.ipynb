{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LIcmFyp4PDvE"
   },
   "outputs": [],
   "source": [
    "# 필수 라이브러리 임포트\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    RobertaForSequenceClassification, \n",
    "    RobertaTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    GenerationConfig, \n",
    "    pipeline, \n",
    "    TextStreamer, \n",
    "    TextIteratorStreamer\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import faiss\n",
    "from boltons.iterutils import chunked\n",
    "from tqdm import tqdm\n",
    "from keybert import KeyBERT\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import langchain\n",
    "from langchain import HuggingFacePipeline, PromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.llms.utils import enforce_stop_tokens\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.output_parsers import PydanticOutputParser, OutputFixingParser, RetryWithErrorOutputParser\n",
    "from langchain.output_parsers.format_instructions import PYDANTIC_FORMAT_INSTRUCTIONS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "from functools import partial\n",
    "from threading import Thread\n",
    "from typing import List, Mapping, Optional, Any, Dict\n",
    "from pydantic import BaseModel, Field, validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1712475524457,
     "user": {
      "displayName": "­방선유(엘텍공과대학 소프트웨어학부)",
      "userId": "17256880122841329967"
     },
     "user_tz": -540
    },
    "id": "rVNp5JNMO57T",
    "outputId": "62a0ea73-b11d-410f-ca2b-820730ddf160"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# CUDA 사용 가능 여부 확인\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KhGX-5WvUpBH"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plQntoeFO5nN"
   },
   "source": [
    "# (1) Text Emotion Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5081,
     "status": "ok",
     "timestamp": 1712475529499,
     "user": {
      "displayName": "­방선유(엘텍공과대학 소프트웨어학부)",
      "userId": "17256880122841329967"
     },
     "user_tz": -540
    },
    "id": "5yZTbuJMPKFh",
    "outputId": "19b07a57-ce61-4ec6-e9dd-0042a8f90c50"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 감정 라벨 정의\n",
    "emotion_to_label = {\n",
    "    0: 'anger', 1: 'disgust', 2: 'fear', 3: 'sadness', 4: 'contentment', \n",
    "    5: 'excitement', 6: 'awe', 7: 'amusement'\n",
    "}\n",
    "\n",
    "# RoBERTa 모델 및 토크나이저 불러오기\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=len(emotion_to_label))\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# 모델을 GPU로 이동\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# 모델 파일 경로\n",
    "model_path = \"./0ver2_SOTA_ED_model_comp_0.8973.pt\"\n",
    "\n",
    "# 저장된 모델 불러오기\n",
    "state_dict = torch.load(model_path)\n",
    "\n",
    "# 모델에 state_dict 적용\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ptm1kVFSPbvQ"
   },
   "outputs": [],
   "source": [
    "# 데이터셋 클래스 정의\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=128):\n",
    "        self.data = pd.DataFrame(data)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        utterance = self.data['utterance'].iloc[idx]\n",
    "        text = f\"{utterance}\"\n",
    "\n",
    "        encoding = self.tokenizer(text, max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt')\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'],\n",
    "            'attention_mask': encoding['attention_mask']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV 파일을 DataFrame으로 읽기\n",
    "df = pd.read_csv(\"./example_data.csv\")\n",
    "\n",
    "df_dataset = EmotionDataset(df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jdJCZS8pPdjH"
   },
   "outputs": [],
   "source": [
    "# 예측 수행\n",
    "def predict_label(inputs):\n",
    "    with torch.no_grad():\n",
    "        inputs['input_ids'] = inputs['input_ids'].to(DEVICE)\n",
    "        inputs['attention_mask'] = inputs['attention_mask'].to(DEVICE)\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_label_id = logits.argmax(dim=1).item()\n",
    "    predicted_label = emotion_to_label[predicted_label_id]\n",
    "    return predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1712475533836,
     "user": {
      "displayName": "­방선유(엘텍공과대학 소프트웨어학부)",
      "userId": "17256880122841329967"
     },
     "user_tz": -540
    },
    "id": "z73Db4TuPml-",
    "outputId": "542a0578-ac08-4593-fbe5-fab2a58f6f1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My wife made me pancakes for breakfast. I have a full belly and feel rather happy now.\n",
      "contentment\n"
     ]
    }
   ],
   "source": [
    "predictions = [predict_label(inputs) for inputs in df_dataset]\n",
    "\n",
    "df['emotion'] = predictions\n",
    "print(df['utterance'].iloc[0])\n",
    "print(df['emotion'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red' size='60pt'> 사용할 예시 텍스트: </font>\n",
    "\n",
    "## My wife made me pancakes for breakfast. I have a full belly and feel rather happy now.\n",
    "\n",
    "# <font color='red' size='60pt'> 예측된 감정: </font>\n",
    "\n",
    "## <font color='red'> Contentment </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8P0-qvlcURs3"
   },
   "source": [
    "# (2) Tokenization and Keyphrase Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NiLdosNfW339"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n"
     ]
    }
   ],
   "source": [
    "# keybert로 key phrase 추출\n",
    "# KeyBERT 모델 초기화\n",
    "kw_model = KeyBERT()\n",
    "\n",
    "def filter_pos_with_stopwords(text):\n",
    "  keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1,5),use_maxsum = True,top_n = 5)\n",
    "  sorted_keywords = sorted(keywords, key=lambda x: x[1], reverse=True)\n",
    "  return sorted_keywords[0][0]\n",
    "\n",
    "df['keyphrase'] = df['utterance'].apply(filter_pos_with_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OP-CC7ACUSIn"
   },
   "outputs": [],
   "source": [
    "# 불용어 제거 전 축약어 전처리\n",
    "def expand_contractions(text):\n",
    "    contractions = {\n",
    "        \"he's\": \"he is\",\n",
    "        \"He's\": \"He is\",\n",
    "        \"she's\": \"she is\",\n",
    "        \"She's\": \"She is\",\n",
    "        \"I've\": \"I have\",\n",
    "        \"i've\": \"i have\",\n",
    "        \"you've\": \"you have\",\n",
    "        \"You've\": \"You have\",\n",
    "        \"we've\": \"we have\",\n",
    "        \"We've\": \"We have\",\n",
    "        \"they've\": \"they have\",\n",
    "        \"They've\": \"They have\",\n",
    "        \"she'll\": \"she will\",\n",
    "        \"She'll\": \"She will\",\n",
    "        \"he'll\": \"he will\",\n",
    "        \"He'll\": \"He will\",\n",
    "        \"don't\": \"do not\",\n",
    "        \"Don't\": \"Do not\",\n",
    "        \"doesn't\": \"does not\",\n",
    "        \"Doesn't\": \"Does not\",\n",
    "        \"didn't\": \"did not\",\n",
    "        \"Didn't\": \"Did not\",\n",
    "        \"shouldn't\": \"should not\",\n",
    "        \"Shouldn't\": \"Should not\",\n",
    "        \"wouldn't\": \"would not\",\n",
    "        \"Wouldn't\": \"Would not\",\n",
    "        \"I'll\": \"I will\",\n",
    "        \"i'll\": \"i will\",\n",
    "        \"i'd\": \"i would\",\n",
    "        \"I'd\": \"I would\",\n",
    "        \"you'd\": \"you would\",\n",
    "        \"You'd\": \"You would\",\n",
    "        \"we'd\": \"we would\",\n",
    "        \"We'd\": \"We would\",\n",
    "        \"they'd\": \"they would\",\n",
    "        \"They'd\": \"They would\",\n",
    "        \"i'm\": \"i am\",\n",
    "        \"I'm\": \"I am\",\n",
    "        \"you're\": \"you are\",\n",
    "        \"You're\": \"You are\",\n",
    "        \"it's\": \"it is\",\n",
    "        \"It's\": \"It is\",\n",
    "        \"we're\": \"we are\",\n",
    "        \"We're\": \"We are\",\n",
    "        \"they're\": \"they are\",\n",
    "        \"They're\": \"They are\",\n",
    "        \"isn't\": \"is not\",\n",
    "        \"Isn't\": \"Is not\",\n",
    "        \"aren't\": \"are not\",\n",
    "        \"Aren't\": \"Are not\",\n",
    "        \"wasn't\": \"was not\",\n",
    "        \"Wasn't\": \"Was not\",\n",
    "        \"weren't\": \"were not\",\n",
    "        \"Weren't\": \"Were not\",\n",
    "        \"hasn't\": \"has not\",\n",
    "        \"Hasn't\": \"Has not\",\n",
    "        \"haven't\": \"have not\",\n",
    "        \"Haven't\": \"Have not\",\n",
    "        \"hadn't\": \"had not\",\n",
    "        \"Hadn't\": \"Had not\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"Won't\": \"Will not\",\n",
    "        \"wouldn't\": \"would not\",\n",
    "        \"Wouldn't\": \"Would not\",\n",
    "        \"can't\": \"can not\",\n",
    "        \"Can't\": \"Can not\",\n",
    "        \"couldn't\": \"could not\",\n",
    "        \"Couldn't\": \"Could not\",\n",
    "        \"shouldn't\": \"should not\",\n",
    "        \"Shouldn't\": \"Should not\",\n",
    "        \"mightn't\": \"might not\",\n",
    "        \"Mightn't\": \"Might not\",\n",
    "        \"mustn't\": \"must not\",\n",
    "        \"Mustn't\": \"Must not\",\n",
    "        \"there's\": \"there is\",\n",
    "        \"There's\": \"There is\",\n",
    "        \"they're\": \"they are\",\n",
    "        \"They're\": \"They are\",\n",
    "        \"who's\": \"who is\",\n",
    "        \"Who's\": \"Who is\",\n",
    "        \"what's\": \"what is\",\n",
    "        \"What's\": \"What is\",\n",
    "        \"where's\": \"where is\",\n",
    "        \"Where's\": \"Where is\",\n",
    "        \"here's\": \"here is\",\n",
    "        \"Here's\": \"Here is\",\n",
    "        \"when's\": \"when is\",\n",
    "        \"When's\": \"When is\",\n",
    "        \"why's\": \"why is\",\n",
    "        \"Why's\": \"Why is\",\n",
    "        \"how's\": \"how is\",\n",
    "        \"How's\": \"How is\",\n",
    "        \"ain't\": \"are not\",\n",
    "        \"can't've\": \"cannot have\",\n",
    "        \"cause\": \"because\",\n",
    "        \"could've\": \"could have\",\n",
    "        \"couldn't've\": \"could not have\",\n",
    "        \"hadn't've\": \"had not have\",\n",
    "        \"hasn't've\": \"has not have\",\n",
    "        \"haven't've\": \"have not have\",\n",
    "        \"he'd've\": \"he would have\",\n",
    "        \"he'll've\": \"he will have\",\n",
    "        \"he's\": \"he is\",\n",
    "        \"how'd\": \"how did\",\n",
    "        \"how'd'y\": \"how do you\",\n",
    "        \"how'll\": \"how will\",\n",
    "        \"how's\": \"how is\",\n",
    "        \"i'd\": \"I would\",\n",
    "        \"i'd've\": \"I would have\",\n",
    "        \"i'll\": \"I will\",\n",
    "        \"i'll've\": \"I will have\",\n",
    "        \"i'm\": \"I am\",\n",
    "        \"i've\": \"I have\",\n",
    "        \"isn't\": \"is not\",\n",
    "        \"it'd\": \"it would\",\n",
    "        \"it'd've\": \"it would have\",\n",
    "        \"it'll\": \"it will\",\n",
    "        \"it'll've\": \"it will have\",\n",
    "        \"it's\": \"it is\",\n",
    "        \"let's\": \"let us\",\n",
    "        \"ma'am\": \"madam\",\n",
    "        \"mayn't\": \"may not\",\n",
    "        \"might've\": \"might have\",\n",
    "        \"mightn't\": \"might not\",\n",
    "        \"mightn't've\": \"might not have\",\n",
    "        \"must've\": \"must have\",\n",
    "        \"mustn't\": \"must not\",\n",
    "        \"mustn't've\": \"must not have\",\n",
    "        \"needn't\": \"need not\",\n",
    "        \"needn't've\": \"need not have\",\n",
    "        \"o'clock\": \"of the clock\",\n",
    "        \"oughtn't\": \"ought not\",\n",
    "        \"oughtn't've\": \"ought not have\",\n",
    "        \"shan't\": \"shall not\",\n",
    "        \"sha'n't\": \"shall not\",\n",
    "        \"shan't've\": \"shall not have\",\n",
    "        \"she'd\": \"she would\",\n",
    "        \"she'd've\": \"she would have\",\n",
    "        \"she'll\": \"she will\",\n",
    "        \"she'll've\": \"she will have\",\n",
    "        \"she's\": \"she is\",\n",
    "        \"should've\": \"should have\",\n",
    "        \"shouldn't\": \"should not\",\n",
    "        \"shouldn't've\": \"should not have\",\n",
    "        \"so've\": \"so have\",\n",
    "        \"so's\": \"so is\",\n",
    "        \"that'd\": \"that would\",\n",
    "        \"that'd've\": \"that would have\",\n",
    "        \"that's\": \"that is\",\n",
    "        \"there'd\": \"there would\",\n",
    "        \"there'd've\": \"there would have\",\n",
    "        \"there's\": \"there is\",\n",
    "        \"they'd\": \"they would\",\n",
    "        \"they'd've\": \"they would have\",\n",
    "        \"they'll\": \"they will\",\n",
    "        \"they'll've\": \"they will have\",\n",
    "        \"they're\": \"they are\",\n",
    "        \"they've\": \"they have\",\n",
    "        \"to've\": \"to have\",\n",
    "        \"wasn't\": \"was not\",\n",
    "        \"we'd\": \"we would\",\n",
    "        \"we'd've\": \"we would have\",\n",
    "        \"we'll\": \"we will\",\n",
    "        \"we'll've\": \"we will have\",\n",
    "        \"we're\": \"we are\",\n",
    "        \"we've\": \"we have\",\n",
    "        \"wen't\": \"were not\",\n",
    "        \"weren't\": \"were not\",\n",
    "        \"what'll\": \"what will\",\n",
    "        \"what'll've\": \"what will have\",\n",
    "        \"what're\": \"what are\",\n",
    "        \"what's\": \"what is\",\n",
    "        \"what've\": \"what have\",\n",
    "        \"when's\": \"when is\",\n",
    "        \"when've\": \"when have\",\n",
    "        \"where'd\": \"where did\",\n",
    "        \"where's\": \"where is\",\n",
    "        \"where've\": \"where have\",\n",
    "        \"who'll\": \"who will\",\n",
    "        \"who'll've\": \"who will have\",\n",
    "        \"who's\": \"who is\",\n",
    "        \"who've\": \"who have\",\n",
    "        \"why's\": \"why is\",\n",
    "        \"why've\": \"why have\",\n",
    "        \"will've\": \"will have\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"won't've\": \"will not have\",\n",
    "        \"would've\": \"would have\",\n",
    "        \"wouldn't\": \"would not\",\n",
    "        \"wouldn't've\": \"would not have\",\n",
    "        \"y'all\": \"you all\",\n",
    "        \"y'all'd\": \"you all would\",\n",
    "        \"y'all'd've\": \"you all would have\",\n",
    "        \"y'all're\": \"you all are\",\n",
    "        \"y'all've\": \"you all have\",\n",
    "        \"you'd\": \"you had\",\n",
    "        \"you'd've\": \"you would have\",\n",
    "        \"you'll\": \"you will\",\n",
    "        \"you'll've\": \"you will have\",\n",
    "        \"you're\": \"you are\",\n",
    "        \"you've\": \"you have\",\n",
    "        \"!\": \",\",\n",
    "        \"....\": \",\",\n",
    "        \"...\": \",\",\n",
    "        \"..\": \",\",\n",
    "        \".\": \",\",\n",
    "        \"*\": \" \",\n",
    "        \"-\": \" \",\n",
    "        \",,,,,\": \",\",\n",
    "        \",,,,\": \",\",\n",
    "        \",,,\": \",\",\n",
    "        \",,\": \",\",\n",
    "\n",
    "    }\n",
    "\n",
    "    for contraction, expansion in contractions.items():\n",
    "        text = text.replace(contraction, expansion)\n",
    "    return text\n",
    "\n",
    "df['T5_T2'] = df['utterance'].apply(expand_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 587,
     "status": "ok",
     "timestamp": 1712475536293,
     "user": {
      "displayName": "­방선유(엘텍공과대학 소프트웨어학부)",
      "userId": "17256880122841329967"
     },
     "user_tz": -540
    },
    "id": "jVsWCI-WWoop",
    "outputId": "d294af31-168e-495f-9ee3-0fa009b9cfff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/a2070064/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/a2070064/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 불용어 제거\n",
    "# NLTK 불용어 목록을 불러오기\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# 불용어 제거 함수\n",
    "def remove_stopwords(text):\n",
    "    # 텍스트 소문자로 변환\n",
    "    text = text.lower()\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_sentence = [word for word in word_tokens if not word in stop_words]\n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "# 'utterance' 열에 불용어 제거 함수 적용\n",
    "df['T5_T2'] = df['T5_T2'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YRb2FK8iXiq1"
   },
   "outputs": [],
   "source": [
    "# 특수문자 제거\n",
    "def expand_contractions2(text):\n",
    "    contractions = {\n",
    "        \",,,,,,\": \",\",\n",
    "        \",,,,,\": \",\",\n",
    "        \",,,,\": \",\",\n",
    "        \",,,\": \",\",\n",
    "        \",,\": \",\",\n",
    "        \",  ,\": \",\",\n",
    "        \", ,\": \",\",\n",
    "        \"    \": \" \",\n",
    "        \"   \": \" \",\n",
    "        \"  \": \" \",\n",
    "        \" ,\": \",\",\n",
    "    }\n",
    "    for contraction, expansion in contractions.items():\n",
    "        text = text.replace(contraction, expansion)\n",
    "    return text\n",
    "\n",
    "df['T5_T2'] = df['T5_T2'].apply(expand_contractions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1712475536294,
     "user": {
      "displayName": "­방선유(엘텍공과대학 소프트웨어학부)",
      "userId": "17256880122841329967"
     },
     "user_tz": -540
    },
    "id": "NLUgNaqHXIpd",
    "outputId": "e8580a30-4ea4-4735-fdb5-ce023128b25d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wife made pancakes, breakfast, full belly, feel rather happy, made pancakes\n",
      "contentment, wife made pancakes, breakfast, full belly, feel rather happy, made pancakes\n"
     ]
    }
   ],
   "source": [
    "df['E1_T5_T2'] = df['emotion'] + \", \" + df['T5_T2'] + \", \" + df['keyphrase']\n",
    "print(df['T5_T2'].iloc[0] + \", \" + df['keyphrase'].iloc[0])\n",
    "print(df['E1_T5_T2'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red' size='60pt'> 토큰화 및 중요 어구 추출 후 Append: </font>\n",
    "\n",
    "## wife made pancakes, breakfast, full belly, feel rather happy, made pancakes\n",
    "\n",
    "# <font color='red' size='60pt'> 감정 라벨 Prepend: </font>\n",
    "\n",
    "## <font color='red'>contentment</font>, wife made pancakes, breakfast, full belly, feel rather happy, made pancakes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jOtjEOYePrL_"
   },
   "source": [
    "# (3) Append 3 synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "li4UhUwNPo8F"
   },
   "outputs": [],
   "source": [
    "# JSON 파일에서 동의어 데이터 불러오기\n",
    "with open('./emotion_synonyms.json', 'r', encoding='utf-8') as file:\n",
    "    synonyms = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G-YtRVo7P0l3"
   },
   "outputs": [],
   "source": [
    "from angle_emb import AnglE\n",
    "\n",
    "# 사전 학습된 AnglE 모델을 불러오기\n",
    "uae = AnglE.from_pretrained('WhereIsAI/UAE-Large-V1', pooling_strategy='cls').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WZPz_9yQP7D9"
   },
   "outputs": [],
   "source": [
    "# AnglE 이용해 벡터로 인코딩, faiss 인덱스를 설정\n",
    "def get_index(emotion):\n",
    "    np.random.seed(1234)\n",
    "    d = 1024  # 벡터의 차원 수\n",
    "    index = faiss.IndexFlatL2(d) # L2 거리(유클리드 거리) 메트릭을 사용하는 faiss 인덱스 초기화\n",
    "\n",
    "    chunk_size = 64\n",
    "    synonyms_list = synonyms.get(emotion, [])  # 해당 감정에 대한 동의어 목록 불러오기\n",
    "\n",
    "    # 동의어 목록을 청크 단위로 나누어 인코딩 후 Faiss 인덱스에 벡터를 추가\n",
    "    for chunk in tqdm(chunked(synonyms_list, chunk_size)):\n",
    "        for vec in uae.encode(chunk):\n",
    "            index.add(vec.reshape(-1, d))\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jV5tMtyrVIZ7"
   },
   "outputs": [],
   "source": [
    "# 감정에 대한 faiss 인덱스에서 입력 벡터와 가장 유사한 상위 3개의 벡터를 검색하는 함수\n",
    "def get_top3_synonyms(emotion, text):\n",
    "    vec = uae.encode(text)  # 입력 텍스트를 벡터로 인코딩\n",
    "    synonyms_list = synonyms.get(emotion, [])\n",
    "    append_synonyms = ''  # 결과 동의어 문자열 초기화\n",
    "\n",
    "     # 입력 벡터와 가장 유사한 상위 3개의 벡터 검색\n",
    "    D, I = get_index(emotion).search(vec, 3) # D는 검색된 거리를, I는 해당 인덱스의 ID를 반환\n",
    "    for id in I[0]:\n",
    "      if append_synonyms=='':\n",
    "        append_synonyms += synonyms_list[id]\n",
    "      else: append_synonyms += ', ' + synonyms_list[id]\n",
    "\n",
    "    print(append_synonyms)\n",
    "\n",
    "    return append_synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1712475544668,
     "user": {
      "displayName": "­방선유(엘텍공과대학 소프트웨어학부)",
      "userId": "17256880122841329967"
     },
     "user_tz": -540
    },
    "id": "NIMgmWC0VUKP",
    "outputId": "a4f5fdf5-3547-4d2d-ece4-3fe8efd93f3d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 56.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gratification, satiation, light heartedness\n",
      "contentment, wife made pancakes, breakfast, full belly, feel rather happy, made pancakes, gratification, satiation, light heartedness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df['E1_T5_T2_E4'] = df.apply(lambda row: f\"{row['E1_T5_T2']}, {get_top3_synonyms(row['emotion'], row['utterance'])}\", axis=1)\n",
    "print(df['E1_T5_T2_E4'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red' size='60pt'> 감정 라벨의 동의어 중 원본 문장과 유사도 높은 3개 Append: </font>\n",
    "\n",
    "## contentment, wife made pancakes, breakfast, full belly, feel rather happy, made pancakes <font color='red'> gratification, satiation, light heartedness</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ngy8b3pZUbZ5"
   },
   "source": [
    "# (4) Append Style Modifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 모델 다운로드 및 설정\n",
    "def load_embeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    return HuggingFaceEmbeddings(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chroma 데이터베이스 초기화\n",
    "def initialize_chroma(persist_directory, embeddings):\n",
    "    return Chroma(persist_directory=persist_directory, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama 모델 및 토크나이저 로드\n",
    "def load_model_and_tokenizer(model_name_or_path, model_basename, device):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "    model = AutoGPTQForCausalLM.from_quantized(\n",
    "        model_name_or_path,\n",
    "        revision=\"gptq-4bit-128g-actorder_True\",\n",
    "        model_basename=model_basename,\n",
    "        use_safetensors=True,\n",
    "        trust_remote_code=True,\n",
    "        inject_fused_attention=False,\n",
    "        device=device,\n",
    "        quantize_config=None,\n",
    "    )\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트 생성 함수\n",
    "def generate_prompt(prompt, system_prompt=\"\"):\n",
    "    return f\"\"\"\n",
    "[INST] <>\n",
    "{system_prompt}\n",
    "<>\n",
    "\n",
    "{prompt} [/INST]\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 생성 파이프라인 설정\n",
    "def initialize_text_pipeline(model, tokenizer, streamer):\n",
    "    return pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=1024,\n",
    "        temperature=0,\n",
    "        top_p=0.95,\n",
    "        repetition_penalty=1.15,\n",
    "        streamer=streamer,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 LLama 파이프라인, 데이터베이스, 시스템 프롬프트 템플릿으로 QA 체인을 초기화\n",
    "def initialize_qa_chain(llm, db, system_prompt_template):\n",
    "    prompt = PromptTemplate(template=system_prompt_template, input_variables=[\"context\", \"question\"])\n",
    "    return RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=db.as_retriever(search_kwargs={\"k\": 5}),\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": prompt},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시스템 프롬프트 템플릿\n",
    "SYSTEM_PROMPT = \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\"\n",
    "\n",
    "# 사용자 정의 프롬프트 템플릿\n",
    "USER_PROMPT_TEMPLATE = generate_prompt(\n",
    "    \"\"\"\n",
    "{context}\n",
    "\n",
    "Instruction: Give style modifiers for generating the image of the input TEXT.\n",
    "1. From the given context, select style modifiers to improve the emotion expression, aesthetics and authenticity of the final image.\n",
    "2. Output style modifiers in 3 or less.\n",
    "\n",
    "TEXT: {question}\n",
    "\"\"\",\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nRJnLVv_Y-4C"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/a2070064/demo/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/a2070064/demo/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the given text, here are three style modifiers that could be used to generate an image that captures the emotions and themes present in the text:\n",
      "\n",
      "1. Warm colors: To convey a sense of comfort, happiness, and satisfaction, warm colors such as golden yellow, orange, and red can be used. These colors can be applied to the background, objects, or even the characters' skin tones to create a cozy and inviting atmosphere.\n",
      "2. Soft brush strokes: To emphasize the lightheartedness and playfulness of the scene, soft brush strokes can be used to give the impression of gentle, effortless movements. This can also help to convey a sense of ease and contentment.\n",
      "3. Whimsical patterns: To add a touch of fun and playfulness to the image, whimsical patterns such as polka dots, stripes, or other geometric shapes can be incorporated into the design. These patterns can be used for the characters' clothing, the background, or even the utensils and plates used in the breakfast scene.\n",
      "\n",
      "Here is the output in JSON format:\n",
      "\n",
      "{\"modifiers\": [{\"type\": \"color\", \"value\": [\"golden yellow\", \"orange\", \"red\"]}, {\"type\": \"brush stroke\", \"value\": [\"soft\", \"effortless\"]}, {\"type\": \"pattern\", \"value\": [\"polka dots\", \"stripes\", \"geometric shapes\"]}]}\n"
     ]
    }
   ],
   "source": [
    "# 초기화\n",
    "embeddings = load_embeddings()\n",
    "db = initialize_chroma(persist_directory=\"./styleDB\", embeddings=embeddings)\n",
    "\n",
    "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GPTQ\"\n",
    "model_basename = \"model\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(model_name_or_path, model_basename, device)\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "text_pipeline = initialize_text_pipeline(model, tokenizer, streamer)\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=text_pipeline, model_kwargs={\"temperature\": 0})\n",
    "qa_chain = initialize_qa_chain(llm, db, USER_PROMPT_TEMPLATE)\n",
    "\n",
    "# # 데이터프레임에 새 열 추가\n",
    "df['answer'] = \"\"\n",
    "df['modifiers'] = \"\"\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    answer_format = '\\n* Use this format(JSON) for the answer {\\\"modifiers\\\": []}.' # 답변 포맷 정의\n",
    "    text = row['E1_T5_T2_E4']\n",
    "    df.at[index, 'answer'] = qa_chain(f\"{text}{answer_format}\").get('result') # QA 체인을 통해 답변 생성 및 데이터프레임에 저장\n",
    "\n",
    "print(df['answer'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[INST] <>\\nUse the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n<>\\n\\n\\n{\"description\": \"The JSON file contains style-modifiers list categorized as \\'color\\', \\'dimensionality\\', \\'light\\', \\'perspective\\', and \\'style\\'. In each category, modifiers are separated by commas.\", \"modifiers\": [{\"color\": [\"White\", \"Black\", \"Brown\", \"Light Gray\", \"Gray\", \"Dark Gray\", \"Maroon\", \"Red\", \"Orange\", \"Yellow\", \"Lime\", \"Green\", \"Cyan\", \"Teal\", \"Blue\", \"Indigo\", \"Purple\", \"Violet\", \"Fuchsia\", \"Magenta\", \"Pink\", \"Tan\", \"Beige\", \"Blush\", \"Scarlet\", \"Olive Green\", \"Chartreuse\", \"Turquoise\", \"Aqua\", \"Azure\", \"Dark White\", \"Dark Brown\", \"Dark Maroon\", \"Dark Red\", \"Dark Orange\", \"Dark Yellow\", \"Dark Lime\", \"Dark Green\", \"Dark Cyan\", \"Dark Blue\", \"Dark Purple\", \"Dark Magenta\", \"Dark Pink\", \"Light Black\", \"Light Brown\", \"Light Maroon\", \"Light Red\", \"Light Orange\", \"Light Yellow\", \"Light Lime\", \"Light Green\", \"Light Cyan\", \"Light Blue\", \"Light Purple\", \"Light Magenta\", \"Light Pink\", \"Vivid Brown\", \"Vivid Maroon\", \"Vivid Red\", \"Vivid Orange\", \"Vivid Yellow\", \"Vivid Lime\", \"Vivid Green\",\\n\\n\"Crooked\", \"Cubism\", \"Cubo-Expressionism\", \"Cubo-Futurism\", \"Dadaism\", \"Damask Patterns\", \"Detailed\", \"Diptych\", \"Divine\", \"Divisionism\", \"Drop Art\", \"Droste Effect\", \"Dutch Golden Age\", \"Early Renaissance\", \"Eccentrism\", \"Edge-To-Edge\", \"Elegance\", \"Elegant\", \"Elite\", \"Emotion-Color Synesthesia\", \"Entropy\", \"Epic Composition\", \"Escapism\", \"Existential\", \"Existentialism\", \"Exploded-View Diagram\", \"Expressionism\", \"Extreme\", \"Extreme Bubble Design\", \"Fauvism\", \"Feynman Diagram\", \"Figurative Expressionism\", \"Flat Design\", \"Flower of Life\", \"Fluxus Art Movement\", \"Folk Art\", \"Foreshortening\", \"Fourier Art\", \"Fractal Art\", \"Frasurbane\", \"Generic\", \"Girih Patterns\", \"Glassmorphism\", \"Gothic\", \"Gothic Horror\", \"Graffiti By Anthony Lister\", \"Graffiti By Artur Bordalo\", \"Graffiti By Banksy\", \"Graffiti By Caledonia Curry\", \"Graffiti By Eduardo Kobra\", \"Graffiti By Futura\", \"Graffiti By Invader\", \"Graffiti By Leonard Hilton McGurr\", \"Graffiti By Mr. Brainwash\", \"Graffiti By Os Gemeos\",\\n\\n\"Stuckism\", \"Sumatraism\", \"Suprematism\", \"Surface Detail\", \"Symbolism\", \"Synchronism\", \"Synthetic Cubism\", \"Synthetism\", \"Tachisme\", \"Tactile Design\", \"Teleidoscope\", \"Temporary Art\", \"Tiger Pattern\", \"Tint\", \"Tonalism\", \"Topographic\", \"Traditional Art\", \"Transautomatism\", \"Trippy\", \"Triptych\", \"Tryptamine\", \"Tubism\", \"Ukiyo-e\", \"Ultra Quality\", \"Uon.visuals\", \"Vedute\", \"Verism\", \"Vienna Secession\", \"Voronoi\", \"Vorticism\", \"Warhol\", \"White Noise\", \"Wuhtercuhler\", \"Y2K Design\", \"Zellij Patterns\"]}]}\\n\\n\"Vivid Green\", \"Vivid Cyan\", \"Vivid Blue\", \"Vivid Purple\", \"Vivid Magenta\", \"Vivid Pink\", \"Vibrant Colors\", \"Vivid\", \"Bright Colors\", \"Light Colors\", \"Dark Colors\", \"Darkened\", \"Dingy Colors\", \"Variegated\", \"Pure\", \"Purity\", \"Faded Colors\", \"Faded\", \"Happy Colors\", \"Exciting Colors\", \"Gloomy Colors\", \"Warm Color Palette\", \"Cool Color Palette\", \"Inverted Colors\", \"Colorful\", \"Multicolored\", \"Rainbow\", \"Spectral Color\", \"Vibrant\", \"Saturated\", \"High Saturation\", \"Low Saturation\", \"Neon\", \"Electric Colors\", \"ComplimentaryColors\", \"Light\", \"Light Mode\", \"Dark\", \"Dark Mode\", \"Tones of Black\", \"Tones of Black in Background\", \"Light Blue Background\", \"Light Blue Foreground\", \"Monochromatic\", \"Monochrome\", \"Black and White\", \"Grayscale\", \"Desaturated\", \"High Contrast\", \"Low Contrast\"], \"dimensionality\": [\"2-Dimensional\", \"2D\", \"2.5-Dimensional\", \"2.5D\", \"3-Dimensional\", \"3D\", \"Overdimensional\", \"Underdimensional\", \"Hyperdimensional\", \"Subdimensional\", \"Everdimensional\", \"Omnidimensional\",\\n\\nPerspective\", \"Divergent Perspective\", \"Cross-Section\", \"Cutaway\", \"Cutaway-View\", \"Cutaway Drawing\", \"Exploded-View\", \"Exploded-View Drawing\"], \"style\": [\"1100s\", \"1200s\", \"1300s\", \"1400s\", \"1500s\", \"1600s\", \"1700s\", \"1800s\", \"1900s\", \"1920s Decor\", \"1930s Decor\", \"1940s Decor\", \"1950s Decor\", \"1960s Decor\", \"1970s Decor\", \"1980s Decor\", \"1990s Decor\", \"2000s Decor\", \"2000s Pattern\", \"2010s Decor\", \"2020s Decor\", \"Abstract Expressionism\", \"Academicism\", \"Action Painting\", \"Amate\", \"Ambiguous Art\", \"Anachronism\", \"Analytical Cubism\", \"Anti-Design\", \"Appealing\", \"Art Brut\", \"Art By Alberto Giacometti\", \"Art By Alexander Milne Calder\", \"Art By Anne Geddes\", \"Art By Anne McCaffrey\", \"Art By Arthur Kopcke\", \"Art By Artofethan\", \"Art By Genpei Akasegawa\", \"Art By George Grosz\", \"Art By George Maciunas\", \"Art By Georges Ribemont-Dessaignes\", \"Art By Gustav Klimt\", \"Art By H.R. Giger\", \"Art By Hannah Hoch\", \"Art By Ilia Zdanevich\", \"Art By Jean Tinguely\", \"Art By Jim Burns\", \"Art By\\n\\nInstruction: Give style modifiers for generating the image of the input TEXT.\\n1. From the given context, select style modifiers to improve the emotion expression, aesthetics and authenticity of the final image.\\n2. Output style modifiers in 3 or less.\\n\\nTEXT: contentment, wife made pancakes breakfast, full belly feel rather happy,, wife pancakes breakfast, gratification, satiation, light heartedness\\n* Use this format(JSON) for the answer {\"modifiers\": []}.\\n [/INST]  Based on the given text, here are three style modifiers that could be used to generate an image that captures the emotions and themes present in the text:\\n\\n1. Warm colors: To convey a sense of comfort, happiness, and satisfaction, warm colors such as golden yellow, orange, and red can be used. These colors can be applied to the background, objects, or even the characters\\' skin tones to create a cozy and inviting atmosphere.\\n2. Soft brush strokes: To emphasize the lightheartedness and playfulness of the scene, soft brush strokes can be used to give the impression of gentle, effortless movements. This can also help to convey a sense of ease and contentment.\\n3. Whimsical patterns: To add a touch of fun and playfulness to the image, whimsical patterns such as polka dots, stripes, or other geometric shapes can be incorporated into the design. These patterns can be used for the characters\\' clothing, the background, or even the utensils and plates used in the breakfast scene.\\n\\nHere is the output in JSON format:\\n\\n{\"modifiers\": [{\"type\": \"color\", \"value\": [\"golden yellow\", \"orange\", \"red\"]}, {\"type\": \"brush stroke\", \"value\": [\"soft\", \"effortless\"]}, {\"type\": \"pattern\", \"value\": [\"polka dots\", \"stripes\", \"geometric shapes\"]}]}'"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['answer'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pGidZVD0ZJ93"
   },
   "outputs": [],
   "source": [
    "def get_array (text) :\n",
    "    index = text.find(\"[/INST]\")\n",
    "    if index != -1:  # 찾은 경우\n",
    "        text = text[index + len(\"[/INST]\"):]\n",
    "        \n",
    "    start_index = text.find('[')\n",
    "\n",
    "    end_index = text.rfind(']') + 1\n",
    "\n",
    "    # 배열 부분 추출\n",
    "    array_part = text[start_index:end_index].strip()\n",
    "\n",
    "    return array_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jnpYr0hvZKg2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"type\": \"color\", \"value\": [\"golden yellow\", \"orange\", \"red\"]}, {\"type\": \"brush stroke\", \"value\": [\"soft\", \"effortless\"]}, {\"type\": \"pattern\", \"value\": [\"polka dots\", \"stripes\", \"geometric shapes\"]}]\n"
     ]
    }
   ],
   "source": [
    "df['modifiers'] = df['answer'].apply(get_array)\n",
    "print(df['modifiers'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vKfjLDZmZMGz"
   },
   "outputs": [],
   "source": [
    "def get_modifiers (string) :\n",
    "    # 제거할 요소들\n",
    "    elements_to_remove = [\"modifiers\", \"artist\", \"color\", \"dimensionality\", \"style\", \"light\", \"modifier\", \"category\", \"perspective\", \"value\", \"typography\", \"type\", \"hand-written\", \"textual\"]\n",
    "    string = string.replace(\"\\\\n\", '')\n",
    "    try:\n",
    "        json_data = json.loads(string)\n",
    "        values = [list(obj.values())[1] for obj in json_data]\n",
    "        values = [item for item in values if item not in elements_to_remove]\n",
    "        result_string = \", \".join(values)\n",
    "        result_string = re.sub(r'(?i)typography', '', result_string)\n",
    "        return result_string\n",
    "    except Exception as e:\n",
    "        print(f'org: {string}')\n",
    "\n",
    "        # 제거할 요소들을 문자열에서 삭제하는 반복문\n",
    "        for element in elements_to_remove:\n",
    "            string = string.replace('\"' + element + '\"', ' ')\n",
    "\n",
    "        # 제거할 문자들\n",
    "        characters_to_remove = r'[,:\\[\\]\\{\\}\\\\;]'\n",
    "\n",
    "        # 문자열에서 지정된 문자들을 모두 제거\n",
    "        string = re.sub(characters_to_remove, '', string)\n",
    "\n",
    "        extracted_list = re.findall(r'\"(.*?)\"', string)\n",
    "        # 추출된 값을 필터링하여 요소 제거\n",
    "        values = [item.strip() for item in extracted_list if item.lower().strip() not in elements_to_remove]\n",
    "\n",
    "        result_string = \", \".join(values)\n",
    "        result_string = re.sub(r'(?i)typography', '', result_string)\n",
    "\n",
    "        print(result_string)\n",
    "\n",
    "        return result_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pH4eUbvJZRI3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org: [{\"type\": \"color\", \"value\": [\"golden yellow\", \"orange\", \"red\"]}, {\"type\": \"brush stroke\", \"value\": [\"soft\", \"effortless\"]}, {\"type\": \"pattern\", \"value\": [\"polka dots\", \"stripes\", \"geometric shapes\"]}]\n",
      "golden yellow, orange, red, brush stroke, soft, effortless, pattern, polka dots, stripes, geometric shapes\n"
     ]
    }
   ],
   "source": [
    "df['modifiers'] = df['modifiers'].apply(get_modifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'golden yellow, brush stroke, soft'"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "style_list = list(df['modifiers'].iloc[0].split(','))\n",
    "style_list = str(style_list[0] +\",\"+ style_list[3] +\",\"+ style_list[4])\n",
    "style_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contentment, wife made pancakes, breakfast, full belly, feel rather happy, made pancakes, gratification, satiation, light heartedness, golden yellow, brush stroke, soft\n"
     ]
    }
   ],
   "source": [
    "df['prompt'] = df['E1_T5_T2_E4'] + ', ' + style_list\n",
    "print(df['prompt'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red' size='60pt'> 감정 라벨과 연관 높은 스타일 키워드 3개 Append: </font>\n",
    "\n",
    "## contentment,wife made pancakes, breakfast, full belly, feel rather happy, made pancakes, gratification, satiation, light heartedness, <font color='red'>golden yellow, brush stroke, soft\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Demo Kernel",
   "language": "python",
   "name": "demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
